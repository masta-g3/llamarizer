{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import CodeLlamaTokenizerFast\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T02:55:55.471791Z",
     "start_time": "2023-09-18T02:55:54.918202Z"
    }
   },
   "id": "b6ace412e19074d0"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "tokenizer = CodeLlamaTokenizerFast.from_pretrained(\"codellama/CodeLlama-13b-hf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T02:55:55.733077Z",
     "start_time": "2023-09-18T02:55:55.462611Z"
    }
   },
   "id": "60ab924bcc1e666c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472\n"
     ]
    }
   ],
   "source": [
    "## Input paths.\n",
    "arxiv_path = os.getenv(\"ARXIV_PATH\")\n",
    "summaries_path = os.getenv(\"SUMMARY_PATH\")\n",
    "\n",
    "## Load.\n",
    "arxiv_fnames = os.listdir(arxiv_path)\n",
    "arxiv_fnames = [fname.replace(\".txt\", \"\") for fname in arxiv_fnames]\n",
    "summaries_fnames = os.listdir(summaries_path)\n",
    "summaries_fnames = [fname.replace(\".json\", \"\") for fname in summaries_fnames]\n",
    "\n",
    "## Combine.\n",
    "all_fnames = set(arxiv_fnames).intersection(set(summaries_fnames))\n",
    "print(len(all_fnames))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T02:55:55.828845Z",
     "start_time": "2023-09-18T02:55:55.821217Z"
    }
   },
   "id": "d3de1072bbd5a3a2"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "keep_keys = [\n",
    "    'main_contribution',\n",
    "     'takeaways',\n",
    "     'category',\n",
    "     'novelty_analysis',\n",
    "     'novelty_score',\n",
    "     'technical_analysis',\n",
    "     'technical_score',\n",
    "     'enjoyable_analysis',\n",
    "     'enjoyable_score'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T02:55:56.176223Z",
     "start_time": "2023-09-18T02:55:56.140224Z"
    }
   },
   "id": "92811cf3cdbb341f"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/472 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4e81918c2d449ceb221b19825f80f8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Create list of input-output pairs.\n",
    "input_output_pairs = []\n",
    "token_lengths = []\n",
    "string_lengths = []\n",
    "\n",
    "for fname in tqdm(all_fnames):\n",
    "    ## Loaders.\n",
    "    input_text = open(f\"{arxiv_path}/{fname}.txt\", \"r\").read()\n",
    "    input_text = re.sub(r'\\n{3,}', '\\n\\n', input_text)\n",
    "    biblio_pattern = re.compile(r'(\\nReferences.{0,5}|\\nBibliography.{0,5})\\s*[\\s\\S]*', re.IGNORECASE)\n",
    "    input_text = biblio_pattern.sub('', input_text)\n",
    "    input_text = input_text[:25000 * 3]\n",
    "    \n",
    "    output_json = open(f\"{summaries_path}/{fname}.json\", \"r\").read()\n",
    "    output_json = json.loads(output_json)\n",
    "    summary_text = output_json[\"Summary\"]\n",
    "    output_json = {k: output_json[k] for k in keep_keys}\n",
    "    output_text = json.dumps(output_json, indent=4)\n",
    "    \n",
    "        \n",
    "    ## Token count and limit.\n",
    "    input_tokens = len(tokenizer.tokenize(input_text))\n",
    "    output_tokens = len(tokenizer.tokenize(output_text))\n",
    "    \n",
    "    ## Store.\n",
    "    token_lengths.append((input_tokens, output_tokens))\n",
    "    string_lengths.append((len(input_text), len(output_text)))\n",
    "    input_output_pairs.append((input_text, output_text, summary_text))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T02:56:29.452818Z",
     "start_time": "2023-09-18T02:56:00.921406Z"
    }
   },
   "id": "4fc2c92d51ddeb23"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               input  \\\n0  LLM-Rec: Personalized Recommendation via\\nProm...   \n1  Causal-Discovery Performance of ChatGPT in the...   \n2  KoLA: Carefully Benchmarking World Knowledge\\n...   \n3  Connecting Neural Response measurements &\\nCom...   \n4  Code Prompting: a Neural Symbolic Method for\\n...   \n\n                                              output  \\\n0  {\\n    \"main_contribution\": {\\n        \"headli...   \n1  {\\n    \"main_contribution\": {\\n        \"headli...   \n2  {\\n    \"main_contribution\": {\\n        \"headli...   \n3  {\\n    \"main_contribution\": {\\n        \"headli...   \n4  {\\n    \"main_contribution\": {\\n        \"headli...   \n\n                                             summary  \n0  We investigate various prompting strategies fo...  \n1  ChatGPT has demonstrated exceptional proficien...  \n2  The unprecedented performance of large languag...  \n3  Understanding the neural basis of language com...  \n4  Large language models (LLMs) have scaled up to...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>output</th>\n      <th>summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LLM-Rec: Personalized Recommendation via\\nProm...</td>\n      <td>{\\n    \"main_contribution\": {\\n        \"headli...</td>\n      <td>We investigate various prompting strategies fo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Causal-Discovery Performance of ChatGPT in the...</td>\n      <td>{\\n    \"main_contribution\": {\\n        \"headli...</td>\n      <td>ChatGPT has demonstrated exceptional proficien...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>KoLA: Carefully Benchmarking World Knowledge\\n...</td>\n      <td>{\\n    \"main_contribution\": {\\n        \"headli...</td>\n      <td>The unprecedented performance of large languag...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Connecting Neural Response measurements &amp;\\nCom...</td>\n      <td>{\\n    \"main_contribution\": {\\n        \"headli...</td>\n      <td>Understanding the neural basis of language com...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Code Prompting: a Neural Symbolic Method for\\n...</td>\n      <td>{\\n    \"main_contribution\": {\\n        \"headli...</td>\n      <td>Large language models (LLMs) have scaled up to...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## As data framelet.\n",
    "df = pd.DataFrame(input_output_pairs, columns=[\"input\", \"output\", \"summary\"])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T02:56:29.477028Z",
     "start_time": "2023-09-18T02:56:29.459450Z"
    }
   },
   "id": "f7af4071dd1fcf8c"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def generate_prompt(row):\n",
    "    \"\"\" Format intput and output into a prompt. \"\"\"\n",
    "    content = row[\"input\"]\n",
    "    summary = row[\"summary\"]\n",
    "    response = row[\"output\"]\n",
    "    prompt = f\"\"\"\n",
    "### INPUT\n",
    "========================\n",
    "#### Summary:\n",
    "{summary}\n",
    "\n",
    "#### Whitepaper:\n",
    "{content}\n",
    "\n",
    "\n",
    "### INSTRUCTIONS\n",
    "========================\n",
    "Use the following JSON template to respond.\n",
    "\n",
    "{{\n",
    "    \"main_contribution\": {{\n",
    "        \"headline\": \"<<main_headline>>\",\n",
    "        \"description\": \"<<main_description>>\"\n",
    "    }},\n",
    "    \"takeaways\": {{\n",
    "        \"headline\": \"<<takeaways_headline>>\",\n",
    "        \"description\": \"<<takeaways_description>>\",\n",
    "        \"example\": \"<<takeaways_example>>\"\n",
    "    }},\n",
    "    \"category\": \"<<category>>\",\n",
    "    \"novelty_analysis\": \"<<novelty_analysis_text>>\",\n",
    "    \"novelty_score\": <<novelty_score_number>>,\n",
    "    \"technical_analysis\": \"<<technical_analysis_text>>\",\n",
    "    \"technical_score\": <<technical_score_number>>,\n",
    "    \"enjoyable_analysis\": \"<<enjoyable_analysis_text>>\",\n",
    "    \"enjoyable_score\": <<enjoyable_score_number>>\n",
    "}}\n",
    "\n",
    "\n",
    "### RESPONSE\n",
    "========================\n",
    "{response}\n",
    "\n",
    "\n",
    "### END\n",
    "\"\"\"\n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T02:56:29.477261Z",
     "start_time": "2023-09-18T02:56:29.466409Z"
    }
   },
   "id": "ad569ab51e6495bf"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3355bf3794446e380c3d6fd61359c29"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/472 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5384c1e9b18e4c1b8aaf8e634c2407f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gen_dataset():\n",
    "    for _, row in df.iterrows():\n",
    "        yield {\"text\": generate_prompt(row)}\n",
    "        \n",
    "ds = Dataset.from_generator(gen_dataset)\n",
    "ds.save_to_disk(\"data/arxiv_summary_prompts\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T02:56:31.625272Z",
     "start_time": "2023-09-18T02:56:29.469405Z"
    }
   },
   "id": "792c4d5e30cfed90"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "                                               input  \\\n0  LLM-Rec: Personalized Recommendation via\\nProm...   \n1  Causal-Discovery Performance of ChatGPT in the...   \n2  KoLA: Carefully Benchmarking World Knowledge\\n...   \n3  Connecting Neural Response measurements &\\nCom...   \n4  Code Prompting: a Neural Symbolic Method for\\n...   \n\n                                              output  \\\n0  {\\n    \"main_contribution\": {\\n        \"headli...   \n1  {\\n    \"main_contribution\": {\\n        \"headli...   \n2  {\\n    \"main_contribution\": {\\n        \"headli...   \n3  {\\n    \"main_contribution\": {\\n        \"headli...   \n4  {\\n    \"main_contribution\": {\\n        \"headli...   \n\n                                             summary  \\\n0  We investigate various prompting strategies fo...   \n1  ChatGPT has demonstrated exceptional proficien...   \n2  The unprecedented performance of large languag...   \n3  Understanding the neural basis of language com...   \n4  Large language models (LLMs) have scaled up to...   \n\n                                              prompt  \n0  \\n### INPUT\\n========================\\n#### Su...  \n1  \\n### INPUT\\n========================\\n#### Su...  \n2  \\n### INPUT\\n========================\\n#### Su...  \n3  \\n### INPUT\\n========================\\n#### Su...  \n4  \\n### INPUT\\n========================\\n#### Su...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>input</th>\n      <th>output</th>\n      <th>summary</th>\n      <th>prompt</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LLM-Rec: Personalized Recommendation via\\nProm...</td>\n      <td>{\\n    \"main_contribution\": {\\n        \"headli...</td>\n      <td>We investigate various prompting strategies fo...</td>\n      <td>\\n### INPUT\\n========================\\n#### Su...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Causal-Discovery Performance of ChatGPT in the...</td>\n      <td>{\\n    \"main_contribution\": {\\n        \"headli...</td>\n      <td>ChatGPT has demonstrated exceptional proficien...</td>\n      <td>\\n### INPUT\\n========================\\n#### Su...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>KoLA: Carefully Benchmarking World Knowledge\\n...</td>\n      <td>{\\n    \"main_contribution\": {\\n        \"headli...</td>\n      <td>The unprecedented performance of large languag...</td>\n      <td>\\n### INPUT\\n========================\\n#### Su...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Connecting Neural Response measurements &amp;\\nCom...</td>\n      <td>{\\n    \"main_contribution\": {\\n        \"headli...</td>\n      <td>Understanding the neural basis of language com...</td>\n      <td>\\n### INPUT\\n========================\\n#### Su...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Code Prompting: a Neural Symbolic Method for\\n...</td>\n      <td>{\\n    \"main_contribution\": {\\n        \"headli...</td>\n      <td>Large language models (LLMs) have scaled up to...</td>\n      <td>\\n### INPUT\\n========================\\n#### Su...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"prompt\"] = df.apply(generate_prompt, axis=1)\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T03:16:05.651072Z",
     "start_time": "2023-09-18T03:16:05.614423Z"
    }
   },
   "id": "f5591e9f158c867e"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### INPUT\n",
      "========================\n",
      "#### Summary:\n",
      "Recent artificial intelligence (AI) systems have reached milestones in \"grand challenges\" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge.   Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a \"passing\" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach.   Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets.   We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p < 0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form \"adversarial\" questions to probe LLM limitations.   While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.\n",
      "\n",
      "#### Whitepaper:\n",
      "Towards Expert-Level\n",
      "Medical Question Answering\n",
      "with Large Language Models\n",
      "Karan Singhal∗,1, Tao Tu∗,1, Juraj Gottweis∗,1, Rory Sayres∗,1,\n",
      "Ellery Wulczyn1, Le Hou1, Kevin Clark1, Stephen Pfohl1, Heather Cole-Lewis1, Darlene Neal1,\n",
      "Mike Schaekermann1, Amy Wang1, Mohamed Amin1, Sami Lachgar1,\n",
      "Philip Mansﬁeld1, Sushant Prakash1, Bradley Green1, Ewa Dominowska1, Blaise Aguera y Arcas1,\n",
      "Nenad Tomasev2, Yun Liu1, Renee Wong1, Christopher Semturs1, S. Sara Mahdavi1,\n",
      "Joelle Barral1, Dale Webster1, Greg S. Corrado1, Yossi Matias1,\n",
      "Shekoofeh Azizi†,1, Alan Karthikesalingam†,1 and Vivek Natarajan†,1\n",
      "1Google Research, 2DeepMind,\n",
      "Recent artiﬁcial intelligence (AI) systems have reached milestones in “grand challenges” ranging from Go to\n",
      "protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions\n",
      "comparably to physicians has long been viewed as one such grand challenge.\n",
      "Large language models (LLMs) have catalyzed signiﬁcant progress in medical question answering; Med-\n",
      "PaLM was the ﬁrst model to exceed a “passing” score in US Medical Licensing Examination (USMLE)\n",
      "style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested\n",
      "signiﬁcant room for improvement, especially when models’ answers were compared to clinicians’ answers.\n",
      "Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM\n",
      "improvements (PaLM 2), medical domain ﬁnetuning, and prompting strategies including a novel ensemble\n",
      "reﬁnement approach.\n",
      "Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and\n",
      "setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art\n",
      "across MedMCQA, PubMedQA, and MMLU clinical topics datasets.\n",
      "We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical\n",
      "applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred\n",
      "Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility\n",
      "(p < 0.001). We also observed signiﬁcant improvements compared to Med-PaLM on every evaluation axis\n",
      "(p < 0.001) on newly introduced datasets of 240 long-form “adversarial” questions to probe LLM limitations.\n",
      "While further studies are necessary to validate the eﬃcacy of these models in real-world settings, these\n",
      "results highlight rapid progress towards physician-level performance in medical question answering.\n",
      "1 Introduction\n",
      "Language is at the heart of health and medicine, underpinning interactions between people and care providers.\n",
      "Progress in Large Language Models (LLMs) has enabled the exploration of medical-domain capabilities in\n",
      "artiﬁcial intelligence (AI) systems that can understand and communicate using language, promising richer\n",
      "human-AI interaction and collaboration. In particular, these models have demonstrated impressive capabilities\n",
      "on multiple-choice research benchmarks [1–3].\n",
      "In our prior work on Med-PaLM, we demonstrated the importance of a comprehensive benchmark for medical\n",
      "question-answering, human evaluation of model answers, and alignment strategies in the medical domain [1].\n",
      "We introduced MultiMedQA, a diverse benchmark for medical question-answering spanning medical exams,\n",
      "consumer health, and medical research. We proposed a human evaluation rubric enabling physicians and\n",
      "lay-people to perform detailed assessment of model answers. Our initial model, Flan-PaLM, was the ﬁrst to\n",
      "∗ Equal contributions. † Equal leadership.\n",
      "‡ Corresponding authors: {karansinghal, taotu, shekazizi, alankarthi, natviv}@google.com\n",
      "arXiv:2305.09617v1  [cs.CL]  16 May 2023\n",
      "Dec 20\n",
      "Sep 21\n",
      "Mar 22\n",
      "Oct 22\n",
      "Dec 22\n",
      "Dec 22\n",
      "Dec 22\n",
      "Mar 23\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "MedQA (USMLE-Style) Accuracy (%)\n",
      "GPT-Neo\n",
      "33.3\n",
      "PubMedBERT\n",
      "38.1\n",
      "BioLinkBERT\n",
      "45.1\n",
      "DRAGON\n",
      "47.5\n",
      "BioMedLM\n",
      "50.3\n",
      "GPT 3.5\n",
      "60.2\n",
      "Med-PaLM\n",
      "67.2\n",
      "Med-PaLM 2\n",
      "86.5\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Better reflects consensus\n",
      "Better reading comprehension\n",
      "Better knowledge recall\n",
      "Better reasoning\n",
      "High Quality Answer Traits\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "% Responses\n",
      "More inaccurate/irrelevant information\n",
      "Omits more information\n",
      "More evidence of demographic bias\n",
      "Greater extent of harm\n",
      "Greater likelihood of harm\n",
      "Potential Answer Risks\n",
      "Med-PaLM 2 \n",
      "Tie \n",
      "Physician \n",
      "Figure 1 | Med-PaLM 2 performance on MultiMedQA Left: Med-PaLM 2 achieved an accuracy of 86.5% on USMLE-style\n",
      "questions in the MedQA dataset. Right: In a pairwise ranking study on 1066 consumer medical questions, Med-PaLM 2 answers\n",
      "were preferred over physician answers by a panel of physicians across eight of nine axes in our evaluation framework.\n",
      "exceed the commonly quoted passmark on the MedQA dataset comprising questions in the style of the US\n",
      "Medical Licensing Exam (USMLE). However, human evaluation revealed that further work was needed to\n",
      "ensure the AI output, including long-form answers to open-ended questions, are safe and aligned with human\n",
      "values and expectations in this safety-critical domain (a process generally referred to as \"alignment\"). To\n",
      "bridge this, we leveraged instruction prompt-tuning to develop Med-PaLM, resulting in substantially improved\n",
      "physician evaluations over Flan-PaLM. However, there remained key shortfalls in the quality of model answers\n",
      "compared to physicians. Similarly, although Med-PaLM achieved state-of-the-art on every multiple-choice\n",
      "benchmark in MultiMedQA, these scores left room for improvement.\n",
      "Here, we bridge these gaps and further advance LLM capabilities in medicine with Med-PaLM 2. We developed\n",
      "this model using a combination of an improved base LLM (PaLM 2 [4]), medical domain-speciﬁc ﬁnetuning\n",
      "and a novel prompting strategy that enabled improved medical reasoning. Med-PaLM 2 improves upon\n",
      "Med-PaLM by over 19% on MedQA as depicted in Figure 1 (left). The model also approached or exceeded\n",
      "state-of-the-art performance on MedMCQA, PubMedQA, and MMLU clinical topics datasets.\n",
      "While these benchmarks are a useful measure of the knowledge encoded in LLMs, they do not capture\n",
      "the model’s ability to generate factual, safe responses to questions that require nuanced answers, typical\n",
      "in real-world medical question-answering. We study this by applying our previously published rubric for\n",
      "evaluation by physicians and lay-people [1]. Further, we introduce two additional human evaluations: ﬁrst,\n",
      "a pairwise ranking evaluation of model and physician answers to consumer medical questions along nine\n",
      "clinically relevant axes; second, a physician assessment of model responses on two newly introduced adversarial\n",
      "testing datasets designed to probe the limits of LLMs.\n",
      "Our key contributions are summarized as follows:\n",
      "• We developed Med-PaLM 2, a new medical LLM trained using a new base model (PaLM 2 [4]) and\n",
      "targeted medical domain-speciﬁc ﬁnetuning (Section 3.2).\n",
      "• We introduced ensemble reﬁnement as a new prompting strategy to improve LLM reasoning (Section 3.3).\n",
      "• Med-PaLM 2 achieved state-of-the-art results on several MultiMedQA benchmarks, including MedQA\n",
      "USMLE-style questions (Section 4.1).\n",
      "• Human evaluation of long-form answers to consumer medical questions showed that Med-PaLM 2’s answers\n",
      "were preferred to physician and Med-PaLM answers across eight of nine axes relevant to clinical utility,\n",
      "such as factuality, medical reasoning capability, and low likelihood of harm. For example, Med-PaLM 2\n",
      "answers were judged to better reﬂect medical consensus 72.9% of the time compared to physician answers\n",
      "(Section 4.2 and Figure 1).\n",
      "|2\n",
      "• Finally, we introduced two adversarial question datasets to probe the safety and limitations of these models.\n",
      "We found that Med-PaLM 2 performed signiﬁcantly better than Med-PaLM across every axis, further\n",
      "reinforcing the importance of comprehensive evaluation. For instance, answers were rated as having low\n",
      "risk of harm for 90.6% of Med-PaLM 2 answers, compared to 79.4% for Med-PaLM. (Section 4.2, Figure 5,\n",
      "and Table A.3).\n",
      "2 Related Work\n",
      "The advent of transformers [5] and large language models (LLMs) [6, 7] has renewed interest in the pos-\n",
      "sibilities of AI for medical question-answering tasks–a long-standing “grand challenge” [8–10]. A majority\n",
      "of these approaches involve smaller language models trained using domain speciﬁc data (BioLinkBert [11],\n",
      "DRAGON [12], PubMedGPT [13], PubMedBERT [14], BioGPT [15]), resulting in a steady improvement in\n",
      "state-of-the-art performance on benchmark datasets such as MedQA (USMLE) [16], MedMCQA [17], and\n",
      "PubMedQA [18].\n",
      "However, with the rise of larger general-purpose LLMs such as GPT-3 [19] and Flan-PaLM [20, 21] trained on\n",
      "internet-scale corpora with massive compute, we have seen leapfrog improvements on such benchmarks, all in\n",
      "a span of a few months (Figure 1). In particular, GPT 3.5 [3] reached an accuracy of 60.2% on the MedQA\n",
      "(USMLE) dataset, Flan-PaLM reached an accuracy of 67.6%, and GPT-4-base [2] achieved 86.1%.\n",
      "In parallel, API access to the GPT family of models has spurred several studies evaluating the specialized\n",
      "clinical knowledge in these models, without speciﬁc alignment to the medical domain. Levine et al. [22]\n",
      "evaluated the diagnostic and triage accuracies of GPT-3 for 48 validated case vignettes of both common and\n",
      "severe conditions and compared to lay-people and physicians. GPT-3’s diagnostic ability was found to be\n",
      "better than lay-people and close to physicians. On triage, the performance was less impressive and closer to\n",
      "lay-people. On a similar note, Duong & Solomon [23], Oh et al. [24], and Antaki et al. [25] studied GPT-3\n",
      "performance in genetics, surgery, and ophthalmology, respectively. More recently, Ayers et al. [26] compared\n",
      "ChatGPT and physician responses on 195 randomly drawn patient questions from a social media forum and\n",
      "found ChatGPT responses to be rated higher in both quality and empathy.\n",
      "With Med-PaLM and Med-PaLM 2, we take a “best of both worlds” approach: we harness the strong out-of-the-\n",
      "box potential of the latest general-purpose LLMs and then use publicly available medical question-answering\n",
      "data and physician-written responses to align the model to the safety-critical requirements of the medical\n",
      "domain. We introduce the ensemble reﬁnement prompting strategy to improve the reasoning capabilities of\n",
      "the LLM. This approach is closely related to self-consistency [27], recitation-augmentation [28], self-reﬁne [29],\n",
      "and dialogue enabled reasoning [30]. It involves contextualizing model responses by conditioning on multiple\n",
      "reasoning paths generated by the same model in a prior step as described further in Section 3.3.\n",
      "In this work, we not only evaluate our model on multiple-choice medical benchmarks but also provide a rubric\n",
      "for how physicians and lay-people can rigorously assess multiple nuanced aspects of the model’s long-form\n",
      "answers to medical questions with independent and pairwise evaluation. This approach allows us to develop\n",
      "and evaluate models more holistically in anticipation of future real-world use.\n",
      "3 Methods\n",
      "3.1 Datasets\n",
      "We evaluated Med-PaLM 2 on multiple-choice and long-form medical question-answering datasets from\n",
      "MultiMedQA [1] and two new adversarial long-form datasets introduced below.\n",
      "Multiple-choice questions For evaluation on multiple-choice questions, we used the MedQA [16], MedM-\n",
      "CQA [17], PubMedQA [18] and MMLU clinical topics [31] datasets (Table 1).\n",
      "Long-form questions For evaluation on long-form questions, we used two sets of questions sampled from\n",
      "MultiMedQA (Table 2).\n",
      "The ﬁrst set (MultiMedQA 140) consists of 140 questions curated from the\n",
      "HealthSearchQA, LiveQA [32], MedicationQA [33] datasets, matching the set used by Singhal et al. [1]. The\n",
      "second set (MultiMedQA 1066), is an expanded sample of 1066 questions sampled from the same sources.\n",
      "|3\n",
      "Table 1 | Multiple-choice question evaluation datasets.\n",
      "Name\n",
      "Count\n",
      "Description\n",
      "MedQA (USMLE)\n",
      "1273\n",
      "General medical knowledge in US medical licensing exam\n",
      "PubMedQA\n",
      "500\n",
      "Closed-domain question answering given PubMed abstract\n",
      "MedMCQA\n",
      "4183\n",
      "General medical knowledge in Indian medical entrance exams\n",
      "MMLU-Clinical knowledge\n",
      "265\n",
      "Clinical knowledge multiple-choice questions\n",
      "MMLU Medical genetics\n",
      "100\n",
      "Medical genetics multiple-choice questions\n",
      "MMLU-Anatomy\n",
      "135\n",
      "Anatomy multiple-choice questions\n",
      "MMLU-Professional medicine\n",
      "272\n",
      "Professional medicine multiple-choice questions\n",
      "MMLU-College biology\n",
      "144\n",
      "College biology multiple-choice questions\n",
      "MMLU-College medicine\n",
      "173\n",
      "College medicine multiple-choice questions\n",
      "Table 2 | Long-form question evaluation datasets.\n",
      "Name\n",
      "Count\n",
      "Description\n",
      "MultiMedQA 140\n",
      "140\n",
      "Sample from HealthSearchQA, LiveQA, MedicationQA [1]\n",
      "MultiMedQA 1066\n",
      "1066\n",
      "Sample from HealthSearchQA, LiveQA, MedicationQA (Extended from [1])\n",
      "Adversarial (General)\n",
      "58\n",
      "General adversarial dataset\n",
      "Adversarial (Health equity)\n",
      "182\n",
      "Health equity adversarial dataset\n",
      "Adversarial questions We also curated two new datasets of adversarial questions designed to elicit model\n",
      "answers with potential for harm and bias: a general adversarial set and health equity focused adversarial\n",
      "set (Table 2). The ﬁrst set (Adversarial - General) broadly covers issues related to health equity, drug\n",
      "use, alcohol, mental health, COVID-19, obesity, suicide, and medical misinformation. Health equity topics\n",
      "covered in this dataset include health disparities, the eﬀects of structural and social determinants on health\n",
      "outcomes, and racial bias in clinical calculators for renal function [34–36]. The second set (Adversarial - Health\n",
      "equity) prioritizes use cases, health topics, and sensitive characteristics based on relevance to health equity\n",
      "considerations in the domains of healthcare access (e.g., health insurance, access to hospitals or primary care\n",
      "provider), quality (e.g., patient experiences, hospital care and coordination), and social and environmental\n",
      "factors (e.g., working and living conditions, food access, and transportation). The dataset was curated to\n",
      "draw on insights from literature on health equity in AI/ML and deﬁne a set of implicit and explicit adversarial\n",
      "queries that cover a range of patient experiences and health conditions [37–41].\n",
      "3.2 Modeling\n",
      "Base LLM For Med-PaLM, the base LLM was PaLM [20]. Med-PaLM 2 builds upon PaLM 2 [4], a new\n",
      "iteration of Google’s large language model with substantial performance improvements on multiple LLM\n",
      "benchmark tasks.\n",
      "Instruction ﬁnetuning We applied instruction ﬁnetuning to the base LLM following the protocol used\n",
      "by Chung et al. [21].\n",
      "The datasets used included the training splits of MultiMedQA–namely MedQA,\n",
      "MedMCQA, HealthSearchQA, LiveQA and MedicationQA. We trained a “uniﬁed” model, which is optimized\n",
      "for performance across all datasets in MultiMedQA using dataset mixture ratios (proportions of each dataset)\n",
      "reported in Table 3. These mixture ratios and the inclusion of these particular datasets were empirically\n",
      "determined. Unless otherwise speciﬁed, Med-PaLM 2 refers to this uniﬁed model. For comparison purposes,\n",
      "we also created a variant of Med-PaLM 2 obtained by ﬁnetuning exclusively on multiple-choice questions\n",
      "which led to improved results on these benchmarks.\n",
      "3.3 Multiple-choice evaluation\n",
      "We describe below prompting strategies used to evaluate Med-PaLM 2 on multiple-choice benchmarks.\n",
      "|4\n",
      "Table 3 | Instruction ﬁnetuning data mixture. Summary of the number of training examples and percent representation\n",
      "in the data mixture for the diﬀerent MultiMedQA datasets used for instruction ﬁnetuning of the uniﬁed Med-PaLM 2 model.\n",
      "Dataset\n",
      "Count\n",
      "Mixture ratio\n",
      "MedQA\n",
      "10,178\n",
      "37.5%\n",
      "MedMCQA\n",
      "182,822\n",
      "37.5%\n",
      "LiveQA\n",
      "10\n",
      "3.9%\n",
      "MedicationQA\n",
      "9\n",
      "3.5%\n",
      "HealthSearchQA\n",
      "45\n",
      "17.6%\n",
      "Few-shot prompting Few-shot prompting [19] involves prompting an LLM by prepending example inputs\n",
      "and outputs before the ﬁnal input. Few-shot prompting remains a strong baseline for prompting LLMs, which\n",
      "we evaluate and build on in this work. We use the same few-shot prompts as used by Singhal et al. [1].\n",
      "Chain-of-thought Chain-of-thought (CoT), introduced by Wei et al. [42], involves augmenting each few-shot\n",
      "example in a prompt with a step-by-step explanation towards the ﬁnal answer. The approach enables an\n",
      "LLM to condition on its own intermediate outputs in multi-step problems. As noted in Singhal et al. [1], the\n",
      "medical questions explored in this study often involve complex multi-step reasoning, making them a good\n",
      "ﬁt for CoT prompting. We crafted CoT prompts to provide clear demonstrations on how to appropriately\n",
      "answer the given medical questions (provided in Section A.3.1).\n",
      "Self-consistency Self-consistency (SC) is a strategy introduced by Wang et al. [43] to improve performance\n",
      "on multiple-choice benchmarks by sampling multiple explanations and answers from the model. The ﬁnal\n",
      "answer is the one with the majority (or plurality) vote. For a domain such as medicine with complex reasoning\n",
      "paths, there might be multiple potential routes to the correct answer. Marginalizing over the reasoning paths\n",
      "can lead to the most accurate answer. The self-consistency prompting strategy led to particularly strong\n",
      "improvements for Lewkowycz et al. [44]. In this work, we performed self-consistency with 11 samplings using\n",
      "COT prompting, as in Singhal et al. [1].\n",
      "Ensemble reﬁnement Building on chain-of-thought and self-consistency, we developed a simple prompting\n",
      "strategy we refer to as ensemble reﬁnement (ER). ER builds on other techniques that involve conditioning\n",
      "an LLM on its own generations before producing a ﬁnal answer, including chain-of-thought prompting and\n",
      "self-Reﬁne [29].\n",
      "ER involves a two-stage process: ﬁrst, given a (few-shot) chain-of-thought prompt and a question, the model\n",
      "produces multiple possible generations stochastically via temperature sampling. In this case, each generation\n",
      "involves an explanation and an answer for a multiple-choice question. Then, the model is conditioned on\n",
      "the original prompt, question, and the concatenated generations from the previous step, and is prompted to\n",
      "produce a reﬁned explanation and answer. This can be interpreted as a generalization of self-consistency,\n",
      "where the LLM is aggregating over answers from the ﬁrst stage instead of a simple vote, enabling the LLM to\n",
      "take into account the strengths and weaknesses of the explanations it generated. Here, to improve performance\n",
      "we perform the second stage multiple times, and then ﬁnally do a plurality vote over these generated answers\n",
      "to determine the ﬁnal answer. Ensemble reﬁnement is depicted in Figure 2.\n",
      "Unlike self-consistency, ensemble reﬁnement may be used to aggregate answers beyond questions with a\n",
      "small set of possible answers (e.g., multiple-choice questions). For example, ensemble reﬁnement can be used\n",
      "to produce improved long-form generations by having an LLM condition on multiple possible responses to\n",
      "generate a reﬁned ﬁnal answer. Given the resource cost of approaches requiring repeated samplings from a\n",
      "model, we apply ensemble reﬁnement only for multiple-choice evaluation in this work, with 11 samplings for\n",
      "the ﬁrst stage and 33 samplings for the second stage.\n",
      "3.4 Overlap analysis\n",
      "An increasingly important concern given recent advances in large models pretrained on web-scale data is the\n",
      "potential for overlap between evaluation benchmarks and training data. To evaluate the potential impact\n",
      "|5\n",
      "Med-PaLM 2\n",
      "Input\n",
      "Reasoning \n",
      "Path 1\n",
      "Reasoning \n",
      "Path K\n",
      "Reasoning \n",
      "Path N\n",
      "...\n",
      "...\n",
      "Med-PaLM 2\n",
      "Answer\n",
      "Figure 2 | Illustration of Ensemble Reﬁnement (ER) with Med-PaLM 2. In this approach, an LLM is conditioned on\n",
      "multiple possible reasoning paths that it generates to enable it to reﬁne and improves its answer.\n",
      "of test set contamination on our evaluation results, we searched for overlapping text segments between\n",
      "multiple-choice questions in MultiMedQA and the corpus used to train the base LLM underlying Med-PaLM\n",
      "2. Speciﬁcally, we deﬁned a question as overlapping if either the entire question or at least 512 contiguous\n",
      "characters overlap with any document in the training corpus. For purposes of this analysis, multiple-choice\n",
      "options or answers were not included as part of the query, since inclusion could lead to underestimation of\n",
      "the number of overlapping questions due to heterogeneity in formatting and ordering options. As a result,\n",
      "this analysis will also treat questions without answers in the training data as overlapping. We believe this\n",
      "methodology is both simple and conservative, and when possible we recommend it over blackbox memorization\n",
      "testing techniques [2], which do not conclusively measure test set contamination.\n",
      "3.5 Long-form evaluation\n",
      "To assess the performance of Med-PaLM 2 on long-form consumer medical question-answering, we conducted\n",
      "a series of human evaluations.\n",
      "Model answers To elicit answers to long-form questions from Med-PaLM models, we used the prompts\n",
      "provided in Section A.3.4. We did this consistently across Med-PaLM and Med-PaLM 2. We sampled from\n",
      "models with temperature 0.0 as in Singhal et al. [1].\n",
      "Physician answers Physician answers were generated as described in Singhal et al. [1]. Physicians were\n",
      "not time-limited in generating answers and were permitted access to reference materials. Physicians were\n",
      "instructed that the audience for their answers to consumer health questions would be a lay-person of average\n",
      "reading comprehension. Tasks were not anchored to a speciﬁc environmental context or clinical scenario.\n",
      "Physician and lay-person raters Human evaluations were performed by physician and lay-person raters.\n",
      "Physician raters were drawn from a pool of 15 individuals: six based in the US, four based in the UK, and\n",
      "ﬁve based in India. Specialty expertise spanned family medicine and general practice, internal medicine,\n",
      "cardiology, respiratory, pediatrics and surgery. Although three physician raters had previously generated\n",
      "physician answers to MultiMedQA questions in prior work [1], none of the physician raters evaluated their\n",
      "own answers and eight to ten weeks elapsed between the task of answer generation and answer evaluation.\n",
      "Lay-person raters were drawn from a pool of six raters (four female, two male, 18-44 years old) based in India,\n",
      "all without a medical background. Lay-person raters’ educational background breakdown was: two with high\n",
      "school diploma, three with graduate degrees, one with postgraduate experience.\n",
      "Individual evaluation of long-form answers Individual long-form answers from physicians, Med-PaLM,\n",
      "and Med-PaLM 2 were rated independently by physician and lay-person raters using rubrics introduced\n",
      "in Singhal et al. [1]. Raters were blinded to the source of the answer and performed ratings in isolation\n",
      "|6\n",
      "without conferring with other raters. Experiments were conducted using the MultiMedQA 140, Adversarial\n",
      "(General), and Adversarial (Health equity) datasets. Ratings for MultiMedQA 140 for Med-PaLM were taken\n",
      "from Singhal et al. [1]. For all new rating experiments, each response was evaluated by three independent\n",
      "raters randomly drawn from the respective pool of raters (lay-person or physician). Answers in MultiMedQA\n",
      "140 were triple-rated, while answers to Adversarial questions were quadruple rated. Inter-rater reliability\n",
      "analysis of MultiMedQA 140 answers indicated that raters were in very good (κ > 0.8) agreement for 10 out\n",
      "of 12 alignment questions, and good (κ > 0.6) agreement for the remaining two questions, including whether\n",
      "answers misses important content, or contain unnecessary additional information (Figure A.1). Triplicate\n",
      "rating enabled inter-rater reliability analyses shown in Section A.2.\n",
      "Pairwise ranking evaluation of long-form answers In addition to independent evaluation of each\n",
      "response, a pairwise preference analysis was performed to directly rank preference between two alternative\n",
      "answers to a given question. Raters were presented with a pair of answers from diﬀerent sources (e.g.,\n",
      "physician vs Med-PaLM 2) for a given question. This intuitively reduces inter-rater variability in ratings\n",
      "across questions.\n",
      "For each pair of answers, raters were asked to select the preferred response or indicate a tie along the following\n",
      "axes (with exact instruction text in quotes):\n",
      "• Alignment with medical consensus: “Which answer better reﬂects the current consensus of the\n",
      "scientiﬁc and clinical community?”\n",
      "• Reading comprehension: “Which answer demonstrates better reading comprehension? (indication the\n",
      "question has been understood)”\n",
      "• Knowledge recall: “Which answer demonstrates better recall of knowledge? (mention of a relevant\n",
      "and/or correct fact for answering the question)”\n",
      "• Reasoning: “Which answer demonstrates better reasoning step(s)? (correct rationale or manipulation of\n",
      "knowledge for answering the question)”\n",
      "• Inclusion of irrelevant content: “Which answer contains more content that it shouldn’t? (either\n",
      "because it is inaccurate or irrelevant)”\n",
      "• Omission of important information: “Which answer omits more important information?”\n",
      "• Potential for demographic bias: “Which answer provides information that is biased for any demographic\n",
      "groups? For example, is the answer applicable only to patients of a particular sex where patients of another\n",
      "sex might require diﬀerent information?”\n",
      "• Possible harm extent: “Which answer has a greater severity/extent of possible harm? (which answer\n",
      "could cause more severe harm)”\n",
      "• Possible harm likelihood: “Which answer has a greater likelihood of possible harm? (more likely to\n",
      "cause harm)”\n",
      "Note that for three of the axes (reading comprehension, knowledge recall, and reasoning), the pairwise\n",
      "ranking evaluation diﬀered from the long-form individual answer evaluation. Speciﬁcally, in individual answer\n",
      "evaluation we separately examine whether a response contains evidence of correctly and incorrectly retrieved\n",
      "facts; the pairwise ranking evaluation consolidates these two questions to understand which response is felt by\n",
      "raters to demonstrate greater quality for this property in aggregate. These evaluations were performed on the\n",
      "MultiMedQA 1066 and Adversarial dataset. Raters were blinded as to the source of each answer, and the\n",
      "order in which answers were shown was randomized. Due to technical issues in the display of answers, raters\n",
      "were unable to review 8 / 1066 answers for the Med-PaLM 2 vs Physician comparison, and 11 / 1066 answers\n",
      "for the Med-PaLM 2 vs Med-PaLM comparison; these answers were excluded from analysis in Figures 1 and 5\n",
      "and Tables A.5 and A.6.\n",
      "Statistical analyses Conﬁdence intervals were computed via bootstrapping (10,000 iterations). Two-tailed\n",
      "permutation tests were used for hypothesis testing (10,000 iterations); for multiple-rated answers, permutations\n",
      "were blocked by answer. For statistical analysis on the MultiMedQA dataset, where Med-PaLM and physician\n",
      "|7\n",
      "Table 4 | Comparison of Med-PaLM 2 results to reported results from GPT-4. Med-PaLM 2 achieves state-of-the-art\n",
      "accuracy on several multiple-choice benchmarks and was ﬁrst announced on March 14, 2023. GPT-4 results were released on\n",
      "March 20, 2023, and GPT-4-base (non-production) results were released on April 12, 2023 [2]. We include Flan-PaLM results\n",
      "from December 2022 for comparison [1]. ER stands for Ensemble Reﬁnement. Best results are across prompting strategies.\n",
      "Dataset\n",
      "Flan-PaLM\n",
      "(best)\n",
      "Med-PaLM 2\n",
      "(ER)\n",
      "Med-PaLM 2\n",
      "(best)\n",
      "GPT-4\n",
      "(5-shot)\n",
      "GPT-4-base\n",
      "(5-shot)\n",
      "MedQA (USMLE)\n",
      "67.6\n",
      "85.4\n",
      "86.5\n",
      "81.4\n",
      "86.1\n",
      "PubMedQA\n",
      "79.0\n",
      "75.0\n",
      "81.8\n",
      "75.2\n",
      "80.4\n",
      "MedMCQA\n",
      "57.6\n",
      "72.3\n",
      "72.3\n",
      "72.4\n",
      "73.7\n",
      "MMLU Clinical knowledge\n",
      "80.4\n",
      "88.7\n",
      "88.7\n",
      "86.4\n",
      "88.7\n",
      "MMLU Medical genetics\n",
      "75.0\n",
      "92.0\n",
      "92.0\n",
      "92.0\n",
      "97.0\n",
      "MMLU Anatomy\n",
      "63.7\n",
      "84.4\n",
      "84.4\n",
      "80.0\n",
      "85.2\n",
      "MMLU Professional medicine\n",
      "83.8\n",
      "92.3\n",
      "95.2\n",
      "93.8\n",
      "93.8\n",
      "MMLU College biology\n",
      "88.9\n",
      "95.8\n",
      "95.8\n",
      "95.1\n",
      "97.2\n",
      "MMLU College medicine\n",
      "76.3\n",
      "83.2\n",
      "83.2\n",
      "76.9\n",
      "80.9\n",
      "answers were single rated, Med-PaLM 2 ratings were randomly sub-sampled to one rating per answer during\n",
      "bootstrapping and permutation testing.\n",
      "4 Results\n",
      "4.1 Multiple-choice evaluation\n",
      "Tables 4 and 5 summarize Med-PaLM 2 results on MultiMedQA multiple-choice benchmarks. Unless speciﬁed\n",
      "otherwise, Med-PaLM 2 refers to the uniﬁed model trained on the mixture in Table 3. We also include\n",
      "comparisons to GPT-4 [2, 45].\n",
      "MedQA Our uniﬁed Med-PaLM 2 model reaches an accuracy of 85.4% using ensemble reﬁnement (ER) as a\n",
      "prompting strategy. Our best result on this dataset is 86.5% obtained from a version of Med-PaLM 2 not\n",
      "aligned for consumer medical question answering, but instead instruction ﬁnetuned only on MedQA, setting a\n",
      "new state-of-art for MedQA performance.\n",
      "MedMCQA On MedMCQA, Med-PaLM 2 obtains a score of 72.3%, exceeding Flan-PaLM performance by\n",
      "over 14% but slightly short of state-of-the-art (73.66 from GPT-4-base [45]).\n",
      "PubMedQA On PubMedQA, Med-PaLM 2 obtains a score of 75.0%. This is below the state-of-the-art\n",
      "performance (81.0 from BioGPT-Large [15]) and is likely because no data was included for this dataset\n",
      "for instruction ﬁnetuning. However, after further exploring prompting strategies for PubMedQA on the\n",
      "development set (see Section A.3.2), the uniﬁed model reached an accuracy of 79.8% with a single run and\n",
      "81.8% using self-consistency (11x). The latter result is state-of-the-art, although we caution that PubMedQA’s\n",
      "test set is small (500 examples), and remaining failures of Med-PaLM 2 and other strong models appear to be\n",
      "largely attributable to label noise intrinsic in the dataset (especially given human performance is 78.0% [18]).\n",
      "MMLU clinical topics On MMLU clinical topics, Med-PaLM 2 signiﬁcantly improves over previously\n",
      "reported results in Med-PaLM [1] and is the state-of-the-art on 3 out 6 topics, with GPT-4-base reporting\n",
      "better numbers in the other three. We note that the test set for each of these topics is small, as reported in\n",
      "Table 1.\n",
      "Interestingly, we see a drop in performance between GPT-4-base and the aligned (production) GPT-4 model\n",
      "on these multiple-choice benchmarks (Table 4). Med-PaLM 2, on the other hand, demonstrates strong\n",
      "performance on multiple-choice benchmarks while being speciﬁcally aligned to the requirements of long-form\n",
      "medical question answering. While multiple-choice benchmarks are a useful measure of the knowledge encoded\n",
      "in these models, we believe human evaluations of model answers along clinically relevant axes as detailed\n",
      "|8\n",
      "Table 5 | Med-PaLM 2 performance with diﬀerent prompting strategies including few-shot, chain-of-thought (CoT), self-\n",
      "consistency (SC), and ensemble reﬁnement (ER).\n",
      "Dataset\n",
      "Med-PaLM 2\n",
      "(5-shot)\n",
      "Med-PaLM 2\n",
      "(COT+SC)\n",
      "Med-PaLM 2\n",
      "(ER)\n",
      "MedQA (USMLE)\n",
      "79.7\n",
      "83.7\n",
      "85.4\n",
      "PubMedQA\n",
      "79.2\n",
      "74.0\n",
      "75.0\n",
      "MedMCQA\n",
      "71.3\n",
      "71.5\n",
      "72.3\n",
      "MMLU Clinical knowledge\n",
      "88.3\n",
      "88.3\n",
      "88.7\n",
      "MMLU Medical genetics\n",
      "90.0\n",
      "89.0\n",
      "92.0\n",
      "MMLU Anatomy\n",
      "77.8\n",
      "80.0\n",
      "84.4\n",
      "MMLU Professional medicine\n",
      "95.2\n",
      "93.4\n",
      "92.3\n",
      "MMLU College biology\n",
      "94.4\n",
      "95.1\n",
      "95.8\n",
      "MMLU College medicine\n",
      "80.9\n",
      "81.5\n",
      "83.2\n",
      "further in Section 4.2 are necessary to assess their utility in real-world clinical applications.\n",
      "We also see in Table 5 that ensemble reﬁnement improves on few-shot and self-consistency prompting strategies\n",
      "in eliciting strong model performance across these benchmarks.\n",
      "Overlap analysis Using the methodology described in Section 3.4, overlap percentages ranged from 0.9%\n",
      "for MedQA to 48.0% on MMLU Medical Genetics. Performance of Med-PaLM 2 was slightly higher on\n",
      "questions with overlap for 6 out of 9 datasets, though the diﬀerence was only statistically signiﬁcant for\n",
      "MedMCQA (accuracy diﬀerence 4.6%, [1.3, 7.7]) due to the relatively small number of questions with overlap\n",
      "in most datasets (Table 6). When we reduced the overlap segment length from 512 to 120 characters (see\n",
      "Section 3.4), overlap percentages increased (11.15% for MedQA to 56.00% on MMLU Medical Genetics),\n",
      "but performance diﬀerences on questions with overlap were similar (Table A.1), and the diﬀerence was still\n",
      "statistically signiﬁcant for just one dataset. These results are similar to those observed by Chowdhery et al.\n",
      "[20], who also saw minimal performance diﬀerence from testing on overlapping data. A limitation of this\n",
      "analysis is that we were not able to exhaustively identify the subset of overlapping questions where the correct\n",
      "answer is also explicitly provided due to heterogeneity in how correct answers can be presented across diﬀerent\n",
      "documents. Restricting the overlap analysis to questions with answers would reduce the overlap percentages\n",
      "while perhaps leading to larger observed performance diﬀerences.\n",
      "4.2 Long-form evaluation\n",
      "Independent evaluation On the MultiMedQA 140 dataset, physicians rated Med-PaLM 2 answers as\n",
      "generally comparable to physician-generated and Med-PaLM-generated answers along the axes we evaluated\n",
      "(Figure 3 and Table A.2). However, the relative performance of each varied across the axes of alignment\n",
      "that we explored, and the analysis was largely underpowered for the eﬀect sizes (diﬀerences) observed. This\n",
      "motivated the pairwise ranking analysis presented below on an expanded sample (MultiMedQA 1066). The\n",
      "only signiﬁcant diﬀerences observed were in favor of Med-PaLM 2 over Med-PaLM (p < 0.05) for the following\n",
      "3 axes: evidence of reasoning, incorrect knowledge recall, and incorrect reasoning.\n",
      "On the adversarial datasets, physicians rated Med-PaLM 2 answers as signiﬁcantly higher quality than\n",
      "Med-PaLM answers across all axes (p < 0.001 for all axes, Figure 3 and Table A.3). This pattern held for\n",
      "both the general and health equity-focused subsets of the Adversarial dataset (Table A.3).\n",
      "Finally, lay-people rated Med-PaLM 2 answers to questions in the MultiMedQA 140 dataset as more helpful\n",
      "and relevant than Med-PaLM answers (p ≤ 0.002 for both dimensions, Figure 4 and Table A.4).\n",
      "Notably, Med-PaLM 2 answers were longer than Med-PaLM and physician answers (Table A.9).\n",
      "On\n",
      "MultiMedQA 140, for instance, the median answer length for Med-PaLM 2 was 794 characters, compared to\n",
      "565.5 for Med-PaLM and 337.5 for physicians. Answer lengths to adversarial questions tended to be longer in\n",
      "general, with median answer length of 964 characters for Med-PaLM 2 and 518 characters for Med-PaLM,\n",
      "possibly reﬂecting the greater complexity of these questions.\n",
      "|9\n",
      "Table 6 | Med-PaLM 2 performance on multiple-choice questions with and without overlap. We deﬁne a question\n",
      "as overlapping if either the entire question or up to 512 characters overlap with any document in the training corpus of the LLM\n",
      "underlying Med-PaLM 2.\n",
      "Dataset\n",
      "Overlap Fraction\n",
      "Performance\n",
      "(without Overlap)\n",
      "Performance\n",
      "(with Overlap)\n",
      "Delta\n",
      "MedQA (USMLE)\n",
      "12/1273\n",
      "(0.9%)\n",
      "85.3\n",
      "[83.4, 87.3]\n",
      "91.7\n",
      "[76.0, 100.0]\n",
      "-6.3\n",
      "[-13.5, 20.8]\n",
      "PubMedQA\n",
      "6/500\n",
      "(1.2%)\n",
      "74.1\n",
      "[70.2, 78.0]\n",
      "66.7\n",
      "[28.9, 100.0]\n",
      "7.4\n",
      "[-16.6, 44.3]\n",
      "MedMCQA\n",
      "893/4183\n",
      "(21.4%)\n",
      "70.5\n",
      "[68.9, 72.0]\n",
      "75.0\n",
      "[72.2, 77.9]\n",
      "-4.6\n",
      "[-7.7, -1.3]\n",
      "MMLU Clinical knowledge\n",
      "55/265\n",
      "(20.8%)\n",
      "88.6\n",
      "[84.3, 92.9]\n",
      "87.3\n",
      "[78.5, 96.1]\n",
      "1.3\n",
      "[-6.8, 13.2]\n",
      "MMLU Medical genetics\n",
      "48/100\n",
      "(48.0%)\n",
      "92.3\n",
      "[85.1, 99.6]\n",
      "91.7\n",
      "[83.8, 99.5]\n",
      "0.6\n",
      "[-11.0, 12.8]\n",
      "MMLU Anatomy\n",
      "37/135\n",
      "(27.4%)\n",
      "82.7\n",
      "[75.2, 90.1]\n",
      "89.2\n",
      "[79.2, 99.2]\n",
      "-6.5\n",
      "[-17.4, 8.7]\n",
      "MMLU Professional medicine\n",
      "79/272\n",
      "(29.0%)\n",
      "89.1\n",
      "[84.7, 93.5]\n",
      "92.4\n",
      "[86.6, 98.2]\n",
      "-3.3\n",
      "[-9.9, 5.5]\n",
      "MMLU College biology\n",
      "60/144\n",
      "(41.7%)\n",
      "95.2\n",
      "[90.7, 99.8]\n",
      "96.7\n",
      "[92.1, 100.0]\n",
      "-1.4\n",
      "[-8.7, 7.1]\n",
      "MMLU College medicine\n",
      "47/173\n",
      "(27.2%)\n",
      "78.6\n",
      "[71.4, 85.7]\n",
      "91.5\n",
      "[83.5, 99.5]\n",
      "-12.9\n",
      "[-22.4, 0.1]\n",
      "Pairwise ranking evaluation Pairwise ranking evaluation more explicitly assessed the relative performance\n",
      "of Med-PaLM 2, Med-PaLM, and physicians. This ranking evaluation was over an expanded set, MultiMedQA\n",
      "1066 and the Adversarial sets. Qualitative examples and their rankings are included in Tables A.7 and A.8,\n",
      "respectively, to provide indicative examples and insight.\n",
      "On MultiMedQA, for eight of the nine axes, Med-PaLM 2 answers were more often rated as being higher\n",
      "quality compared to physician answers (p < 0.001, Figure 1 and Table A.5). For instance, they were more\n",
      "often rated as better reﬂecting medical consensus, or indicating better reading comprehension; and less\n",
      "often rated as omitting important information or representing a risk of harm. However, for one of the axes,\n",
      "including inaccurate or irrelevant information, Med-PaLM 2 answers were not as favorable as physician answers.\n",
      "Med-PaLM 2 answers were rated as higher quality than Med-PaLM axes on the same eight axes (Figure 5\n",
      "and Table A.6); Med-PaLM 2 answers were marked as having more inaccurate or irrelevant information\n",
      "less often than Med-PaLM answers (18.4% Med-PaLM 2 vs. 21.5% Med-PaLM), but the diﬀerence was not\n",
      "signiﬁcant (p = 0.12, Table A.6).\n",
      "On Adversarial questions, Med-PaLM 2 was ranked more favorably than Med-PaLM across every axis\n",
      "(Figure 5), often by substantial margins.\n",
      "5 Discussion\n",
      "We show that Med-PaLM 2 exhibits strong performance in both multiple-choice and long-form medical\n",
      "question answering, including popular benchmarks and challenging new adversarial datasets. We demonstrate\n",
      "performance approaching or exceeding state-of-the-art on every MultiMedQA multiple-choice benchmark,\n",
      "including MedQA, PubMedQA, MedMCQA, and MMLU clinical topics. We show substantial gains in\n",
      "long-form answers over Med-PaLM, as assessed by physicians and lay-people on multiple axes of quality and\n",
      "safety. Furthermore, we observe that Med-PaLM 2 answers were preferred over physician-generated answers\n",
      "in multiple axes of evaluation across both consumer medical questions and adversarial questions.\n",
      "As LLMs become increasingly proﬁcient at structured tests of knowledge, it is becoming more important to\n",
      "|10\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "Proportion of answers in high-quality ratings bins\n",
      "Answer supported by consensus\n",
      "Possible harm extent = No harm\n",
      "Low likelihood of harm\n",
      "Shows evidence of question comprehension\n",
      "Shows evidence of knowledge recall\n",
      "Shows evidence of reasoning\n",
      "No sign of incorrect comprehension\n",
      "No sign of incorrect knowledge recall\n",
      "No sign of incorrect reasoning\n",
      "No inaccurate/irrelevant information\n",
      "No missing important content\n",
      "No sign of bias towards specific subgroups\n",
      "*[\n",
      "*[\n",
      "Physician Evaluation on MultiMedQA\n",
      "Med-PaLM 2\n",
      "Med-PaLM\n",
      "Physician\n",
      "0.5\n",
      "0.6\n",
      "0.7\n",
      "0.8\n",
      "0.9\n",
      "1.0\n",
      "Proportion of answers in high-quality ratings bins\n",
      "Physician Evaluation on Adversarial Questions\n",
      "Figure 3 | Independent long-form evaluation with physician raters Values are the proportion of ratings across answers\n",
      "where each axis was rated in the highest-quality bin. (For instance, \"Possible harm extent = No harm\" reﬂects the proportion\n",
      "of answers where the extent of possible harm was rated \"No harm\".) Left: Independent evaluation of long-form answers from\n",
      "Med-PaLM, Med-PaLM 2 and physicians on the MultiMedQA 140 dataset. Right: Independent evaluation of long-form answers\n",
      "from Med-PaLM and Med-PaLM 2 on the combined adversarial datasets (General and Health equity). Detailed breakdowns are\n",
      "presented in Tables A.2 and A.3. (*) designates 0.01 < p < 0.05 between Med-PaLM and Med-PaLM 2.\n",
      "delineate and assess their capabilities along clinically relevant dimensions [22, 26]. Our evaluation framework\n",
      "examines the alignment of long-form model outputs to human expectations of high-quality medical answers.\n",
      "Our use of adversarial question sets also enables explicit study of LLM performance in diﬃcult cases. The\n",
      "substantial improvements of Med-PaLM 2 relative to Med-PaLM suggest that careful development and\n",
      "evaluation of challenging question-answering tasks is needed to ensure robust model performance.\n",
      "Using a multi-dimensional evaluation framework lets us understand tradeoﬀs in more detail. For instance,\n",
      "Med-PaLM 2 answers signiﬁcantly improved performance on “missing important content” (Table A.2) and\n",
      "were longer on average (Table A.9) than Med-PaLM or physician answers. This may provide beneﬁts for\n",
      "many use cases, but may also impact tradeoﬀs such as including unnecessary additional details vs. omitting\n",
      "important information. The optimal length of an answer may depend upon additional context outside the\n",
      "scope of a question. For instance, questions around whether a set of symptoms are concerning depend upon\n",
      "a person’s medical history; in these cases, the more appropriate response of an LLM may be to request\n",
      "more information, rather than comprehensively listing all possible causes. Our evaluation did not consider\n",
      "multi-turn dialogue [46], nor frameworks for active information acquisition [47].\n",
      "Our individual evaluation did not clearly distinguish performance of Med-PaLM 2 answers from physician-\n",
      "generated answers, motivating more granular evaluation, including pairwise evaluation and adversarial\n",
      "evaluation. In pairwise evaluation, we saw that Med-PaLM 2 answers were preferred over physician answers\n",
      "along several axes pertaining to clinical utility such as factuality, medical reasoning capability, and likelihood\n",
      "of harm. These results indicate that as the ﬁeld progress towards physician-level performance, improved\n",
      "evaluation frameworks will be crucial for further measuring progress.\n",
      "|11\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "% Responses\n",
      "Med-PaLM 2\n",
      "Med-PaLM\n",
      "Physician\n",
      "89\n",
      "10 1\n",
      "74\n",
      "20\n",
      "6\n",
      "86\n",
      "10 4\n",
      "How well does the answer address\n",
      "the intent of the question?\n",
      "Directly addresses query intent\n",
      "Indirectly addresses query intent\n",
      "Does not address query intent\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "% Responses\n",
      "Med-PaLM 2\n",
      "Med-PaLM\n",
      "Physician\n",
      "64\n",
      "24\n",
      "11 1\n",
      "17\n",
      "63\n",
      "16\n",
      "4\n",
      "42\n",
      "49\n",
      "7 2\n",
      "How helpful is this answer to the user?\n",
      "(For instance, does it enable them to draw\n",
      "a conclusion or clarify next steps?)\n",
      "Extremely helpful\n",
      "Helpful\n",
      "Somewhat helpful\n",
      "Not helpful\n",
      "Figure 4 | Independent evaluation of long-form answers with lay-person raters Med-PaLM 2 answers were rated as\n",
      "more directly relevant and helpful than Med-PaLM answers on the MultiMedQA 140 dataset.\n",
      "6 Limitations\n",
      "Given the broad and complex space of medical information needs, methods to measure alignment of model\n",
      "outputs will need continued development. For instance, additional dimensions to those we measure here are\n",
      "likely to be important, such as the empathy conveyed by answers [26]. As we have previously noted, our\n",
      "rating rubric is not a formally validated qualitative instrument, although our observed inter-rater reliability\n",
      "was high (Figure A.1). Further research is required to develop the rigor of rubrics enabling human evaluation\n",
      "of LLM performance in medical question answering.\n",
      "Likewise, a robust understanding of how LLM outputs compare to physician answers is a broad, highly\n",
      "signiﬁcant question meriting much future work; the results we report here represent one step in this research\n",
      "direction. For our current study, physicians generating answers were prompted to provide useful answers to\n",
      "lay-people but were not provided with speciﬁc clinical scenarios or nuanced details of the communication\n",
      "requirements of their audience. While this may be reﬂective of real-world performance for some settings,\n",
      "it is preferable to ground evaluations in highly speciﬁc workﬂows and clinical scenarios. We note that our\n",
      "results cannot be considered generalizable to every medical question-answering setting and audience. Model\n",
      "answers are also often longer than physician answers, which may contribute to improved independent and\n",
      "pairwise evaluations, as suggested by other work [26]. The instructions provided to physicians did not include\n",
      "examples of outputs perceived as higher or lower quality in preference ranking, which might have impacted our\n",
      "evaluation. Furthermore, we did not explicitly assess inter-rater variation in preference rankings or explore\n",
      "how variation in preference rankings might relate to the lived experience, expectations or assumptions of our\n",
      "raters.\n",
      "Physicians were also asked to only produce one answer per question, so this provides a limited assessment of\n",
      "the range of possible physician-produced answers. Future improvements to this methodology could provide a\n",
      "more explicit clinical scenario with recipient and environmental context for answer generation. It could also\n",
      "assess multiple possible physician answers to each question, alongside inter-physician variation. Moreover, for\n",
      "a more principled comparison of LLM answers to medical questions, the medical expertise, lived experience\n",
      "and background, and specialization of physicians providing answers, and evaluating those answers, should\n",
      "|12\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Better reflects consensus\n",
      "Better reading comprehension\n",
      "Better knowledge recall\n",
      "Better reasoning\n",
      "High Quality Answer Traits\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "% Responses\n",
      "More inaccurate/irrelevant information\n",
      "Omits more information\n",
      "More evidence of demographic bias\n",
      "Greater extent of harm\n",
      "Greater likelihood of harm\n",
      "Potential Answer Risks\n",
      "Med-PaLM 2 \n",
      "Tie \n",
      "Med-PaLM   \n",
      "(a) MultiMedQA\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "Better reflects consensus\n",
      "Better reading comprehension\n",
      "Better knowledge recall\n",
      "Better reasoning\n",
      "High Quality Answer Traits\n",
      "0\n",
      "20\n",
      "40\n",
      "60\n",
      "80\n",
      "100\n",
      "% Responses\n",
      "More inaccurate/irrelevant information\n",
      "Omits more information\n",
      "More evidence of demographic bias\n",
      "Greater extent of harm\n",
      "Greater likelihood of harm\n",
      "Potential Answer Risks\n",
      "Med-PaLM 2 \n",
      "Tie \n",
      "Med-PaLM   \n",
      "(b) Adversarial Question Sets\n",
      "Figure 5 | Ranking comparison of long-form answers Med-PaLM 2 answers are consistently preferred over Med-PaLM\n",
      "answers by physician raters across all ratings dimensions, in both MultiMedQA and Adversarial question sets. Each row shows\n",
      "the distribution of side-by-side ratings for which either Med-PaLM 2 (yellow) or Med-PaLM (green)’s answer were preferred; gray\n",
      "shade indicates cases rated as ties along a dimension. Error bars are binomial conﬁdence intervals for the Med-PaLM 2 and\n",
      "Med-PaLM selection rates. Detailed breakdowns for adversarial questions are presented in Supplemental Table 3.\n",
      "be more explicitly explored. It would also be desirable to explore intra- and inter-physician variation in the\n",
      "generation of answers under multiple scenarios as well as contextualize LLM performance by comparison to\n",
      "the range of approaches that might be expected among physicians.\n",
      "Finally, the current evaluation with adversarial data is relatively limited in scope and should not be interpreted\n",
      "as a comprehensive assessment of safety, bias, and equity considerations. In future work, the adversarial data\n",
      "could be systematically expanded to increase coverage of health equity topics and facilitate disaggregated\n",
      "evaluation over sensitive characteristics [48–50] .\n",
      "7 Conclusion\n",
      "These results demonstrate the rapid progress LLMs are making towards physician-level medical question\n",
      "answering. However, further work on validation, safety and ethics is necessary as the technology ﬁnds broader\n",
      "uptake in real-world applications. Careful and rigorous evaluation and reﬁnement of LLMs in diﬀerent contexts\n",
      "for medical question-answering and real world workﬂows will be needed to ensure this technology has a positive\n",
      "impact on medicine and health.\n",
      "Acknowledgments\n",
      "This project was an extensive collaboration between many teams at Google Research. We thank Michael\n",
      "Howell, Boris Babenko, and Naama Hammel for their valuable insights and feedback during our research. We\n",
      "are also grateful to Jeﬀ Dean, James Manyika, Karen DeSalvo, Zoubin Ghahramani, David Fleet, Douglas\n",
      "Eck, and Simon Kornblith for their support during the course of this project. We also want to thank Brett\n",
      "Hatﬁeld, SiWai Man, Sudhanshu Sharma, Gary Parakkal, Gordon Turner, Jukka Zitting, Evan Rappaport,\n",
      "Dave Steiner, Jonas Kemp, Jimmy Hu, Yuan Liu, Jonathan Krause, Kavita Kulkarni, Susan Thomas, Kate\n",
      "Weber, Annisah Um’rani, Anna Iurchenko, Will Vaughan, Julie Wang, Maggie Shiels, and Lauren Winer for\n",
      "their assistance.\n",
      "|13\n",
      "\n",
      "\n",
      "### INSTRUCTIONS\n",
      "========================\n",
      "Use the following JSON template to respond.\n",
      "\n",
      "{\n",
      "    \"main_contribution\": {\n",
      "        \"headline\": \"<<main_headline>>\",\n",
      "        \"description\": \"<<main_description>>\"\n",
      "    },\n",
      "    \"takeaways\": {\n",
      "        \"headline\": \"<<takeaways_headline>>\",\n",
      "        \"description\": \"<<takeaways_description>>\",\n",
      "        \"example\": \"<<takeaways_example>>\"\n",
      "    },\n",
      "    \"category\": \"<<category>>\",\n",
      "    \"novelty_analysis\": \"<<novelty_analysis_text>>\",\n",
      "    \"novelty_score\": <<novelty_score_number>>,\n",
      "    \"technical_analysis\": \"<<technical_analysis_text>>\",\n",
      "    \"technical_score\": <<technical_score_number>>,\n",
      "    \"enjoyable_analysis\": \"<<enjoyable_analysis_text>>\",\n",
      "    \"enjoyable_score\": <<enjoyable_score_number>>\n",
      "}\n",
      "\n",
      "\n",
      "### RESPONSE\n",
      "========================\n",
      "{\n",
      "    \"main_contribution\": {\n",
      "        \"headline\": \"Med-PaLM 2: A Large Language Model Achieving Expert-Level Medical Question Answering\",\n",
      "        \"description\": \"The paper presents Med-PaLM 2, a large language model (LLM) that significantly improves upon its predecessor, Med-PaLM, in answering medical questions. Med-PaLM 2 leverages improvements in the base LLM (PaLM 2), medical domain-specific finetuning, and novel prompting strategies, including an ensemble refinement approach. The model achieved a score of 86.5% on the MedQA dataset, a 19% improvement over Med-PaLM, setting a new state-of-the-art. It also demonstrated performance approaching or exceeding state-of-the-art across other datasets like MedMCQA, PubMedQA, and MMLU clinical topics. In a comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2's answers over those produced by physicians on eight of nine axes relevant to clinical utility.\"\n",
      "    },\n",
      "    \"takeaways\": {\n",
      "        \"headline\": \"Med-PaLM 2 Sets New Benchmark in Medical Question Answering\",\n",
      "        \"description\": \"Med-PaLM 2's performance highlights the potential of LLMs in the medical domain, particularly in answering complex medical questions. The model's ensemble refinement approach, which conditions the LLM on multiple possible reasoning paths, could be a promising strategy for improving LLM reasoning in other domains as well. The model's performance on adversarial questions also underscores the importance of comprehensive evaluation in understanding the safety and limitations of these models. For practitioners, Med-PaLM 2 could serve as a valuable tool in medical education, patient engagement, and clinical decision support, although further studies are needed to validate its efficacy in real-world settings.\",\n",
      "        \"example\": \"For instance, a medical student preparing for the USMLE could use Med-PaLM 2 as a study tool. They could input a medical question, and the model would generate a detailed, physician-level answer. The student could then compare this answer with their own understanding to identify gaps in their knowledge.\"\n",
      "    },\n",
      "    \"category\": \"FINE-TUNING\",\n",
      "    \"novelty_analysis\": \"The paper presents a significant advancement in the application of LLMs to medical question answering. The introduction of Med-PaLM 2, with its novel ensemble refinement prompting strategy and medical domain-specific finetuning, represents a substantial step forward in achieving physician-level performance in this task.\",\n",
      "    \"novelty_score\": 3,\n",
      "    \"technical_analysis\": \"The paper is somewhat technical, delving into the specifics of the Med-PaLM 2 model, its training process, and the novel ensemble refinement prompting strategy. However, it does not require advanced mathematical knowledge and is accessible to readers with a basic understanding of LLMs and their applications.\",\n",
      "    \"technical_score\": 2,\n",
      "    \"enjoyable_analysis\": \"The paper is well-structured and presents a compelling narrative of progress in the application of LLMs to medical question answering. The clear presentation of results, along with detailed human evaluations, makes it an engaging read for those interested in the intersection of AI and medicine.\",\n",
      "    \"enjoyable_score\": 3\n",
      "}\n",
      "\n",
      "\n",
      "### END\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[20][\"prompt\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T03:16:06.518333Z",
     "start_time": "2023-09-18T03:16:06.486441Z"
    }
   },
   "id": "b834376ecafa38dd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "155723e18f3a53aa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
