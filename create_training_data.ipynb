{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import CodeLlamaTokenizerFast\n",
    "from tqdm.notebook import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6ace412e19074d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = CodeLlamaTokenizerFast.from_pretrained(\"codellama/CodeLlama-13b-hf\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60ab924bcc1e666c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Input paths.\n",
    "arxiv_path = os.getenv(\"ARXIV_PATH\")\n",
    "summaries_path = os.getenv(\"SUMMARY_PATH\")\n",
    "\n",
    "## Load.\n",
    "arxiv_fnames = os.listdir(arxiv_path)\n",
    "arxiv_fnames = [fname.replace(\".txt\", \"\") for fname in arxiv_fnames]\n",
    "summaries_fnames = os.listdir(summaries_path)\n",
    "summaries_fnames = [fname.replace(\".json\", \"\") for fname in summaries_fnames]\n",
    "\n",
    "## Combine.\n",
    "all_fnames = set(arxiv_fnames).intersection(set(summaries_fnames))\n",
    "print(len(all_fnames))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3de1072bbd5a3a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keep_keys = [\n",
    "    'main_contribution',\n",
    "     'takeaways',\n",
    "     'category',\n",
    "     'novelty_analysis',\n",
    "     'novelty_score',\n",
    "     'technical_analysis',\n",
    "     'technical_score',\n",
    "     'enjoyable_analysis',\n",
    "     'enjoyable_score'\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92811cf3cdbb341f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Create list of input-output pairs.\n",
    "input_output_pairs = []\n",
    "token_lengths = []\n",
    "string_lengths = []\n",
    "\n",
    "for fname in tqdm(all_fnames):\n",
    "    ## Loaders.\n",
    "    input_text = open(f\"{arxiv_path}/{fname}.txt\", \"r\").read()\n",
    "    input_text = re.sub(r'\\n{3,}', '\\n\\n', input_text)\n",
    "    biblio_pattern = re.compile(r'(\\nReferences.{0,5}|\\nBibliography.{0,5})\\s*[\\s\\S]*', re.IGNORECASE)\n",
    "    input_text = biblio_pattern.sub('', input_text)\n",
    "    input_text = input_text[:25000 * 3]\n",
    "    \n",
    "    output_json = open(f\"{summaries_path}/{fname}.json\", \"r\").read()\n",
    "    output_json = json.loads(output_json)\n",
    "    summary_text = output_json[\"Summary\"]\n",
    "    output_json = {k: output_json[k] for k in keep_keys}\n",
    "    output_text = json.dumps(output_json, indent=4)\n",
    "    \n",
    "        \n",
    "    ## Token count and limit.\n",
    "    input_tokens = len(tokenizer.tokenize(input_text))\n",
    "    output_tokens = len(tokenizer.tokenize(output_text))\n",
    "    \n",
    "    ## Store.\n",
    "    token_lengths.append((input_tokens, output_tokens))\n",
    "    string_lengths.append((len(input_text), len(output_text)))\n",
    "    input_output_pairs.append((input_text, output_text, summary_text))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fc2c92d51ddeb23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## As data framelet.\n",
    "df = pd.DataFrame(input_output_pairs, columns=[\"input\", \"output\", \"summary\"])\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7af4071dd1fcf8c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_prompt(row):\n",
    "    \"\"\" Format intput and output into a prompt. \"\"\"\n",
    "    content = row[\"input\"]\n",
    "    summary = row[\"summary\"]\n",
    "    response = row[\"output\"]\n",
    "    prompt = f\"\"\"\n",
    "### INPUT\n",
    "========================\n",
    "#### Summary:\n",
    "{summary}\n",
    "\n",
    "#### Whitepaper:\n",
    "{content}\n",
    "\n",
    "\n",
    "### INSTRUCTIONS\n",
    "========================\n",
    "Based on the whitepaper presented above, answer the following questions:\n",
    "\n",
    "1. What is the `main_contribution` of this paper? (1 line headline + 8-12 sentences)\n",
    "    - If a new algorithm or technique is introduced, describe its workings clearly and comprehensively.\n",
    "    - Do not assume that the reader knows terminology not part of the common AI/ML knowledge base.\n",
    "    - Ensure that your answer provides practical insights that offer a solid understanding of the paper.\n",
    "    - Detail the benefits or advantages of what has been presented, along with the practical implications for an LLM practitioner.\n",
    "    - Do not include anything already discussed in the summary or abstract.\n",
    "\n",
    "2. What is the main `takeaway`? (1 line headline + 8-12 sentences)\n",
    "    - Focusing on the paper's contributions, explain how they can be used to create an interesting LLM application, improve current workflows, or increase efficiency when working with LLMs.\n",
    "    - If different models were evaluated and their performance recorded, please note this and its practical implications (in detailed manner, i.e.: which model is best for what).\n",
    "    - Be very precise, practical and specific as possible. Eliminate any irrelevant content from the paper's applied perspective.\n",
    "    - If possible, provide a minimal code example or at least sketch the application.\n",
    "\n",
    "3. Which category best describes this paper's primary focus? Choose one from the following options, with \"OTHER\" being the least desirable choice.\n",
    "    a. \"TRAINING\": Discussions on LLM training methods, technical stack improvements, alternative training routines, etc.\n",
    "    b. \"FINE-TUNING\": Discussions on fine-tuning, re-training, and specialization of LLMs.\n",
    "    c. \"ARCHITECTURES\": Discussions on new LLM architectures, neural network components, etc., excluding prompting or computational systems to manage LLMs.\n",
    "    d. \"PROMPTING\": Discussions on prompting methods, agent architectures, etc.\n",
    "    e. \"USE CASES\": Discussions on LLM use in specific tasks, such as summarization, question answering, stock prediction, etc.\n",
    "    f. \"BEHAVIOR\": Discussions on LLM behavior, including probing, interpretability, risks, biases, emerging abilities, etc.\n",
    "    g. \"OTHER\": None of the above.\n",
    "\n",
    "4. On a scale from 1 to 3, how novel is this paper? (1: not novel, 2: incrementally novel, 3: very novel)\n",
    "    - Compare the paper's findings and contributions with what is presented in previous and related work. How unique and significant are the findings?\n",
    "    - Be strict and rigorous; few papers should receive a high score.\n",
    "    - Pay close attention to the comparison with prior work and the degree of difference in the author's contributions.\n",
    "\n",
    "5. On a scale from 1 to 3, how technical is this paper? (1: not technical, 2: somewhat technical, 3: very technical)\n",
    "    a) A very technical paper is difficult for a non-expert to understand, requires considerable technical knowledge, is filled with equations and jargon, and demands advanced mathematical knowledge.\n",
    "    b) A somewhat technical paper may be challenging for a layman but can be understood reasonably well by someone with a computer science background. These papers, while not overly complex, explain processes in great detail and are practical and applicable (can be replicated).\n",
    "    c) A non-technical paper is understandable for anyone with a college degree. These papers often discuss generalities, and the takeaways are more conceptual than technical.\n",
    "\n",
    "6. On a scale from 1 to 3, how enjoyable is this paper? (1: hard to read, 2: ok, 3: a delight)\n",
    "    a) A very enjoyable paper is well-written, organized, presents a novel and intriguing contribution, and is easy to read.\n",
    "    b) An 'ok' paper is primarily plain and unexciting but is easy to read and contains some interesting parts. Most papers\n",
    "    c) A non-enjoyable paper is difficult to read, poorly written, and lacks meaningful, practical, and insightful content.\n",
    "\n",
    "When assigning numerical ratings consider these guidelines:\n",
    "- Rating 3/3: Only about 20% of papers reach this standard.\n",
    "- Rating 2/3: Most papers (50%) fall into this category.\n",
    "- Rating 1/3: Around 30% of papers belong to this category.\n",
    "\n",
    "Do not repeat the same comments across different answers.\n",
    "\n",
    "Use the JSON format as in the following examples to respond.\n",
    "\n",
    "{{\n",
    "    \"main_contribution\": {{\n",
    "        \"headline\": \"<<main_headline>>\",\n",
    "        \"description\": \"<<main_description>>\"\n",
    "    }},\n",
    "    \"takeaways\": {{\n",
    "        \"headline\": \"<<takeaways_headline>>\",\n",
    "        \"description\": \"<<takeaways_description>>\",\n",
    "        \"example\": \"<<takeaways_example>>\"\n",
    "    }},\n",
    "    \"category\": \"<<category>>\",\n",
    "    \"novelty_analysis\": \"<<novelty_analysis_text>>\",\n",
    "    \"novelty_score\": <<novelty_score_number>>,\n",
    "    \"technical_analysis\": \"<<technical_analysis_text>>\",\n",
    "    \"technical_score\": <<technical_score_number>>,\n",
    "    \"enjoyable_analysis\": \"<<enjoyable_analysis_text>>\",\n",
    "    \"enjoyable_score\": <<enjoyable_score_number>>\n",
    "}}\n",
    "\n",
    "\n",
    "### RESPONSE\n",
    "========================\n",
    "{response}\n",
    "\"\"\"\n",
    "    return prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ad569ab51e6495bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gen_dataset():\n",
    "    for _, row in df.iterrows():\n",
    "        yield {\"text\": generate_prompt(row)}\n",
    "        \n",
    "ds = Dataset.from_generator(gen_dataset)\n",
    "ds.save_to_disk(\"data/arxiv_summary_prompts\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "792c4d5e30cfed90"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
